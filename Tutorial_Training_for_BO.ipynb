{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32e2ce2",
   "metadata": {},
   "source": [
    "## Train your own models\n",
    "To train you simply need to call `train.train`.\n",
    "We give all necessary code. The most important bits are in the `priors` dir, e.g. `hebo_prior`, it stores the priors\n",
    "with which we train our models.\n",
    "\n",
    "### Training the HEBO+ model, `model_hebo_morebudget_9_unused_features_3.pt`\n",
    "You can train this model on 8 GPUs using `torchrun` or `submitit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4949ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pfns import priors, encoders, utils, bar_distribution, train\n",
    "from ConfigSpace import hyperparameters as CSH\n",
    "\n",
    "config_heboplus = {\n",
    "    \"priordataloader_class_or_get_batch\": priors.get_batch_to_dataloader(\n",
    "        priors.get_batch_sequence(\n",
    "            priors.hebo_prior.get_batch,\n",
    "            priors.utils.sample_num_feaetures_get_batch,\n",
    "        )\n",
    "    ),\n",
    "    \"encoder_generator\": encoders.get_normalized_uniform_encoder(\n",
    "        encoders.get_variable_num_features_encoder(encoders.Linear)\n",
    "    ),\n",
    "    \"emsize\": 512,\n",
    "    \"nhead\": 4,\n",
    "    \"warmup_epochs\": 5,\n",
    "    \"y_encoder_generator\": encoders.Linear,\n",
    "    \"batch_size\": 128,\n",
    "    \"scheduler\": utils.get_cosine_schedule_with_warmup,\n",
    "    \"extra_prior_kwargs_dict\": {\n",
    "        \"num_features\": 18,\n",
    "        \"hyperparameters\": {\n",
    "            \"lengthscale_concentration\": 1.2106559584074301,\n",
    "            \"lengthscale_rate\": 1.5212245992840594,\n",
    "            \"outputscale_concentration\": 0.8452312502679863,\n",
    "            \"outputscale_rate\": 0.3993553245745406,\n",
    "            \"add_linear_kernel\": False,\n",
    "            \"power_normalization\": False,\n",
    "            \"hebo_warping\": False,\n",
    "            \"unused_feature_likelihood\": 0.3,\n",
    "            \"observation_noise\": True,\n",
    "        },\n",
    "    },\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 0.0001,\n",
    "    \"seq_len\": 60,\n",
    "    \"single_eval_pos_gen\": utils.get_uniform_single_eval_pos_sampler(\n",
    "        50, min_len=1\n",
    "    ),  # <function utils.get_uniform_single_eval_pos_sampler.<locals>.<lambda>()>,\n",
    "    \"aggregate_k_gradients\": 2,\n",
    "    \"nhid\": 1024,\n",
    "    \"steps_per_epoch\": 1024,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"train_mixed_precision\": False,\n",
    "    \"efficient_eval_masking\": True,\n",
    "    \"nlayers\": 12,\n",
    "}\n",
    "\n",
    "\n",
    "config_heboplus_userpriors = {\n",
    "    **config_heboplus,\n",
    "    \"priordataloader_class_or_get_batch\": priors.get_batch_to_dataloader(\n",
    "        priors.get_batch_sequence(\n",
    "            priors.hebo_prior.get_batch,\n",
    "            priors.condition_on_area_of_opt.get_batch,\n",
    "            priors.utils.sample_num_feaetures_get_batch,\n",
    "        )\n",
    "    ),\n",
    "    \"style_encoder_generator\": encoders.get_normalized_uniform_encoder(\n",
    "        encoders.get_variable_num_features_encoder(encoders.Linear)\n",
    "    ),\n",
    "}\n",
    "\n",
    "config_bnn = {\n",
    "    \"priordataloader_class_or_get_batch\": priors.get_batch_to_dataloader(\n",
    "        priors.get_batch_sequence(\n",
    "            priors.simple_mlp.get_batch,\n",
    "            priors.input_warping.get_batch,\n",
    "            priors.utils.sample_num_feaetures_get_batch,\n",
    "        )\n",
    "    ),\n",
    "    \"encoder_generator\": encoders.get_normalized_uniform_encoder(\n",
    "        encoders.get_variable_num_features_encoder(encoders.Linear)\n",
    "    ),\n",
    "    \"emsize\": 512,\n",
    "    \"nhead\": 4,\n",
    "    \"warmup_epochs\": 5,\n",
    "    \"y_encoder_generator\": encoders.Linear,\n",
    "    \"batch_size\": 128,\n",
    "    \"scheduler\": utils.get_cosine_schedule_with_warmup,\n",
    "    \"extra_prior_kwargs_dict\": {\n",
    "        \"num_features\": 18,\n",
    "        \"hyperparameters\": {\n",
    "            \"mlp_num_layers\": CSH.UniformIntegerHyperparameter(\"mlp_num_layers\", 8, 15),\n",
    "            \"mlp_num_hidden\": CSH.UniformIntegerHyperparameter(\n",
    "                \"mlp_num_hidden\", 36, 150\n",
    "            ),\n",
    "            \"mlp_init_std\": CSH.UniformFloatHyperparameter(\n",
    "                \"mlp_init_std\", 0.08896049884896237, 0.1928554813280186\n",
    "            ),\n",
    "            \"mlp_sparseness\": 0.1449806273312999,\n",
    "            \"mlp_input_sampling\": \"uniform\",\n",
    "            \"mlp_output_noise\": CSH.UniformFloatHyperparameter(\n",
    "                \"mlp_output_noise\", 0.00035983014290491186, 0.0013416342770574585\n",
    "            ),\n",
    "            \"mlp_noisy_targets\": True,\n",
    "            \"mlp_preactivation_noise_std\": CSH.UniformFloatHyperparameter(\n",
    "                \"mlp_preactivation_noise_std\",\n",
    "                0.0003145707276259681,\n",
    "                0.0013753183831259406,\n",
    "            ),\n",
    "            \"input_warping_c1_std\": 0.9759720822120248,\n",
    "            \"input_warping_c0_std\": 0.8002534583197192,\n",
    "            \"num_hyperparameter_samples_per_batch\": 16,\n",
    "        },\n",
    "    },\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 0.0001,\n",
    "    \"seq_len\": 60,\n",
    "    \"single_eval_pos_gen\": utils.get_uniform_single_eval_pos_sampler(50, min_len=1),\n",
    "    \"aggregate_k_gradients\": 1,\n",
    "    \"nhid\": 1024,\n",
    "    \"steps_per_epoch\": 1024,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"train_mixed_precision\": True,\n",
    "    \"efficient_eval_masking\": True,\n",
    "}\n",
    "\n",
    "\n",
    "# now let's add the criterions, where we decide the border positions based on the prior\n",
    "def get_ys(config, device=\"cuda:0\"):\n",
    "    bs = 128\n",
    "    all_targets = []\n",
    "    for num_hps in [\n",
    "        2,\n",
    "        8,\n",
    "        12,\n",
    "    ]:  # a few different samples in case the number of features makes a difference in y dist\n",
    "        b = config[\"priordataloader_class_or_get_batch\"].get_batch_method(\n",
    "            bs,\n",
    "            1000,\n",
    "            num_hps,\n",
    "            epoch=0,\n",
    "            device=device,\n",
    "            hyperparameters={\n",
    "                **config[\"extra_prior_kwargs_dict\"][\"hyperparameters\"],\n",
    "                \"num_hyperparameter_samples_per_batch\": -1,\n",
    "            },\n",
    "        )\n",
    "        all_targets.append(b.target_y.flatten())\n",
    "    return torch.cat(all_targets, 0)\n",
    "\n",
    "\n",
    "def add_criterion(config, device=\"cuda:0\"):\n",
    "    return {\n",
    "        **config,\n",
    "        \"criterion\": bar_distribution.FullSupportBarDistribution(\n",
    "            bar_distribution.get_bucket_limits(1000, ys=get_ys(config, device).cpu())\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8789428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dec5326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 384000 y evals to estimate 1000 buckets. Cut off the last 0 ys.\n",
      "Using cpu:0 device\n",
      "init dist\n",
      "Not using distributed\n",
      "DataLoader.__dict__ {'num_steps': 1024, 'get_batch_kwargs': {'batch_size': 128, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7f817c751ee0>, 'seq_len_maximum': 60, 'device': 'cpu:0', 'num_features': 18, 'hyperparameters': {'lengthscale_concentration': 1.2106559584074301, 'lengthscale_rate': 1.5212245992840594, 'outputscale_concentration': 0.8452312502679863, 'outputscale_rate': 0.3993553245745406, 'add_linear_kernel': False, 'power_normalization': False, 'hebo_warping': False, 'unused_feature_likelihood': 0.3, 'observation_noise': True}}, 'num_features': 18, 'epoch_count': 0}\n",
      "Style definition of first 3 examples: None\n",
      "Initialized decoder for standard with (None, 1000)  and nout 1000\n",
      "Using a Transformer with 26.79 M parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(inf,\n",
       " inf,\n",
       " TransformerModel(\n",
       "   (transformer_encoder): TransformerEncoderDiffInit(\n",
       "     (layers): ModuleList(\n",
       "       (0): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (1): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (2): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (3): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (4): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (5): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (6): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (7): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (8): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (9): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (10): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (11): TransformerEncoderLayer(\n",
       "         (self_attn): MultiheadAttention(\n",
       "           (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "         )\n",
       "         (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "         (dropout): Dropout(p=0.0, inplace=False)\n",
       "         (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.0, inplace=False)\n",
       "         (dropout2): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (encoder): Sequential(\n",
       "     (0): Normalize()\n",
       "     (1): VariableNumFeaturesEncoder(\n",
       "       (base_encoder): Linear(in_features=18, out_features=512, bias=True)\n",
       "     )\n",
       "   )\n",
       "   (y_encoder): Linear(in_features=1, out_features=512, bias=True)\n",
       "   (pos_encoder): NoPositionalEncoding()\n",
       "   (decoder_dict): ModuleDict(\n",
       "     (standard): Sequential(\n",
       "       (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "       (1): GELU(approximate='none')\n",
       "       (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "     )\n",
       "   )\n",
       "   (criterion): FullSupportBarDistribution()\n",
       " ),\n",
       " <pfns.priors.utils.get_batch_to_dataloader.<locals>.DL at 0x7f809d906520>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's train either with\n",
    "train.train(**add_criterion(config_heboplus, device=\"cpu:0\"))\n",
    "# or\n",
    "# train.train(**add_criterion(config_heboplus_userpriors))\n",
    "# or\n",
    "# train.train(**add_criterion(config_bnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427f1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
