{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32e2ce2",
   "metadata": {},
   "source": [
    "## Train your own models\n",
    "To train you simply need to call `train.train`.\n",
    "We give all necessary code. The most important bits are in the `priors` dir, e.g. `hebo_prior`, it stores the priors\n",
    "with which we train our models.\n",
    "\n",
    "### Training the HEBO+ model, `model_hebo_morebudget_9_unused_features_3.pt`\n",
    "You can train this model on 8 GPUs using `torchrun` or `submitit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4949ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pfns import priors, encoders, utils, bar_distribution, train\n",
    "from ConfigSpace import hyperparameters as CSH\n",
    "\n",
    "config_heboplus = {\n",
    "     'priordataloader_class_or_get_batch': priors.get_batch_to_dataloader(\n",
    "         priors.get_batch_sequence(\n",
    "             priors.hebo_prior.get_batch,\n",
    "             priors.utils.sample_num_feaetures_get_batch,\n",
    "         )\n",
    "     ),\n",
    "     'encoder_generator': encoders.get_normalized_uniform_encoder(encoders.get_variable_num_features_encoder(encoders.Linear)),\n",
    "     'emsize': 512,\n",
    "     'nhead': 4,\n",
    "     'warmup_epochs': 5,\n",
    "     'y_encoder_generator': encoders.Linear,\n",
    "     'batch_size': 128,\n",
    "     'scheduler': utils.get_cosine_schedule_with_warmup,\n",
    "     'extra_prior_kwargs_dict': {'num_features': 18,\n",
    "      'hyperparameters': {\n",
    "       'lengthscale_concentration': 1.2106559584074301,\n",
    "       'lengthscale_rate': 1.5212245992840594,\n",
    "       'outputscale_concentration': 0.8452312502679863,\n",
    "       'outputscale_rate': 0.3993553245745406,\n",
    "       'add_linear_kernel': False,\n",
    "       'power_normalization': False,\n",
    "       'hebo_warping': False,\n",
    "       'unused_feature_likelihood': 0.3,\n",
    "       'observation_noise': True}},\n",
    "     'epochs': 50,\n",
    "     'lr': 0.0001,\n",
    "     'seq_len': 60,\n",
    "     'single_eval_pos_gen': utils.get_uniform_single_eval_pos_sampler(50, min_len=1), #<function utils.get_uniform_single_eval_pos_sampler.<locals>.<lambda>()>,\n",
    "     'aggregate_k_gradients': 2,\n",
    "     'nhid': 1024,\n",
    "     'steps_per_epoch': 1024,\n",
    "     'weight_decay': 0.0,\n",
    "     'train_mixed_precision': False,\n",
    "     'efficient_eval_masking': True,\n",
    "     'nlayers': 12}\n",
    "\n",
    "\n",
    "config_heboplus_userpriors = {**config_heboplus,\n",
    "    'priordataloader_class_or_get_batch': priors.get_batch_to_dataloader(\n",
    "                              priors.get_batch_sequence(\n",
    "                                  priors.hebo_prior.get_batch,\n",
    "                                  priors.condition_on_area_of_opt.get_batch,\n",
    "                                  priors.utils.sample_num_feaetures_get_batch\n",
    "                              )),\n",
    "    'style_encoder_generator': encoders.get_normalized_uniform_encoder(encoders.get_variable_num_features_encoder(encoders.Linear))\n",
    "}\n",
    "\n",
    "config_bnn = {'priordataloader_class_or_get_batch': priors.get_batch_to_dataloader(\n",
    "             priors.get_batch_sequence(\n",
    "             priors.simple_mlp.get_batch,\n",
    "             priors.input_warping.get_batch,\n",
    "             priors.utils.sample_num_feaetures_get_batch,\n",
    "             priors.hyperparameter_sampling.get_batch,\n",
    "         )\n",
    "     ),\n",
    "     'encoder_generator': encoders.get_normalized_uniform_encoder(encoders.get_variable_num_features_encoder(encoders.Linear)),\n",
    "     'emsize': 512,\n",
    "     'nhead': 4,\n",
    "     'warmup_epochs': 5,\n",
    "     'y_encoder_generator': encoders.Linear,\n",
    "     'batch_size': 128,\n",
    "     'scheduler': utils.get_cosine_schedule_with_warmup,\n",
    "     'extra_prior_kwargs_dict': {'num_features': 18,\n",
    "      'hyperparameters': {'mlp_num_layers': CSH.UniformIntegerHyperparameter('mlp_num_layers', 8, 15),\n",
    "       'mlp_num_hidden': CSH.UniformIntegerHyperparameter('mlp_num_hidden', 36, 150),\n",
    "       'mlp_init_std': CSH.UniformFloatHyperparameter('mlp_init_std',0.08896049884896237, 0.1928554813280186),\n",
    "       'mlp_sparseness': 0.1449806273312999,\n",
    "       'mlp_input_sampling': 'uniform',\n",
    "       'mlp_output_noise': CSH.UniformFloatHyperparameter('mlp_output_noise', 0.00035983014290491186, 0.0013416342770574585),\n",
    "       'mlp_noisy_targets': True,\n",
    "       'mlp_preactivation_noise_std': CSH.UniformFloatHyperparameter('mlp_preactivation_noise_std',0.0003145707276259681, 0.0013753183831259406),\n",
    "       'input_warping_c1_std': 0.9759720822120248,\n",
    "       'input_warping_c0_std': 0.8002534583197192,\n",
    "       'num_hyperparameter_samples_per_batch': 16}\n",
    "                                 },\n",
    "     'epochs': 50,\n",
    "     'lr': 0.0001,\n",
    "     'seq_len': 60,\n",
    "     'single_eval_pos_gen': utils.get_uniform_single_eval_pos_sampler(50, min_len=1), \n",
    "     'aggregate_k_gradients': 1,\n",
    "     'nhid': 1024,\n",
    "     'steps_per_epoch': 1024,\n",
    "     'weight_decay': 0.0,\n",
    "     'train_mixed_precision': True,\n",
    "     'efficient_eval_masking': True,\n",
    "}\n",
    "\n",
    "\n",
    "# now let's add the criterions, where we decide the border positions based on the prior\n",
    "def get_ys(config, device='cuda:0'):\n",
    "    bs = 128\n",
    "    all_targets = []\n",
    "    for num_hps in [2,8,12]: # a few different samples in case the number of features makes a difference in y dist\n",
    "        b = config['priordataloader_class_or_get_batch'].get_batch_method(\n",
    "            bs,1000,  num_hps, epoch=0, device=device, hyperparameters={**config['extra_prior_kwargs_dict']['hyperparameters'],\n",
    "                                                                        'num_hyperparameter_samples_per_batch': -1,})\n",
    "        all_targets.append(b.target_y.flatten())\n",
    "    return torch.cat(all_targets,0)\n",
    "\n",
    "def add_criterion(config, device='cuda:0'):\n",
    "    return {**config, 'criterion': bar_distribution.FullSupportBarDistribution(\n",
    "        bar_distribution.get_bucket_limits(1000,ys=get_ys(config,device).cpu())\n",
    "    )}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dec5326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 384000 y evals to estimate 1000 buckets. Cut off the last 0 ys.\n",
      "Using cpu:0 device\n",
      "init dist\n",
      "Not using distributed\n",
      "DataLoader.__dict__ {'num_steps': 20, 'get_batch_kwargs': {'batch_size': 128, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x118ad4180>, 'seq_len_maximum': 60, 'device': 'cpu:0', 'num_features': 18, 'hyperparameters': {'mlp_num_layers': UniformIntegerHyperparameter(name='mlp_num_layers', default_value=12, meta=None, size=8, lower=8, upper=15, log=False), 'mlp_num_hidden': UniformIntegerHyperparameter(name='mlp_num_hidden', default_value=93, meta=None, size=115, lower=36, upper=150, log=False), 'mlp_init_std': UniformFloatHyperparameter(name='mlp_init_std', default_value=0.1409079900885, meta=None, size=inf, lower=0.088960498849, upper=0.192855481328, log=False), 'mlp_sparseness': 0.1449806273312999, 'mlp_input_sampling': 'uniform', 'mlp_output_noise': UniformFloatHyperparameter(name='mlp_output_noise', default_value=0.00085073221, meta=None, size=inf, lower=0.0003598301429, upper=0.0013416342771, log=False), 'mlp_noisy_targets': True, 'mlp_preactivation_noise_std': UniformFloatHyperparameter(name='mlp_preactivation_noise_std', default_value=0.0008449445554, meta=None, size=inf, lower=0.0003145707276, upper=0.0013753183831, log=False), 'input_warping_c1_std': 0.9759720822120248, 'input_warping_c0_std': 0.8002534583197192, 'num_hyperparameter_samples_per_batch': 16}}, 'num_features': 18, 'epoch_count': 0}\n",
      "Style definition of first 3 examples: None\n",
      "Initialized decoder for standard with (None, 1000)  and nout 1000\n",
      "Using a Transformer with 14.18 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pfns/lib/python3.13/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1744.58s | mean loss  1.20 | pos losses   nan,  nan, 0.91, 1.46, 1.19,  nan,  nan, 1.17, 1.35,  nan,  nan,  nan,  nan, 1.40,  nan, 1.28,  nan, 0.92,  nan, 1.12,  nan,  nan,  nan,  nan, 1.08,  nan,  nan,  nan,  nan, 1.30, 1.15,  nan,  nan,  nan,  nan, 1.14,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 1.20,  nan,  nan, 1.05,  nan, 1.13,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, lr 0.0 data time  0.30 step time 75.97 forward time  1.34 nan share  0.00 ignore share (for classification tasks) 0.0000\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(total_loss=1.1962081611156463, total_positional_losses=[nan, nan, 0.9116353988647461, 1.4590994119644165, 1.186349868774414, nan, nan, 1.169728398323059, 1.350273609161377, nan, nan, nan, nan, 1.3962024450302124, nan, 1.2768378257751465, nan, 0.9184215068817139, nan, 1.1225969791412354, nan, nan, nan, nan, 1.0844190120697021, nan, nan, nan, nan, 1.2964451313018799, 1.146134376525879, nan, nan, nan, nan, 1.1359837055206299, nan, nan, nan, nan, nan, nan, nan, 1.2020983695983887, nan, nan, 1.0488338470458984, nan, 1.134429931640625, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], model=TransformerModel(\n",
       "  (transformer_encoder): TransformerEncoderDiffInit(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): Normalize()\n",
       "    (1): VariableNumFeaturesEncoder(\n",
       "      (base_encoder): Linear(in_features=18, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (y_encoder): Linear(in_features=1, out_features=512, bias=True)\n",
       "  (pos_encoder): NoPositionalEncoding()\n",
       "  (decoder_dict): ModuleDict(\n",
       "    (standard): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): FullSupportBarDistribution()\n",
       "), data_loader=<pfns.priors.utils.get_batch_to_dataloader.<locals>.DL object at 0x147eb86e0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's train either with\n",
    "train.train(**{**add_criterion(config_bnn,device='cpu:0'), 'epochs': 1, 'steps_per_epoch': 20})\n",
    "# or\n",
    "#train.train(**add_criterion(config_heboplus_userpriors))\n",
    "# or\n",
    "#train.train(**add_criterion(config_bnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6bf6195-22c6-4c0c-b983-4bf4511c3dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 384000 y evals to estimate 1000 buckets. Cut off the last 0 ys.\n",
      "Using cpu:0 device\n",
      "init dist\n",
      "Not using distributed\n",
      "DataLoader.__dict__ {'num_steps': 20, 'get_batch_kwargs': {'batch_size': 128, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x118afe980>, 'seq_len_maximum': 60, 'device': 'cpu:0', 'num_features': 18, 'hyperparameters': {'mlp_num_layers': UniformIntegerHyperparameter(name='mlp_num_layers', default_value=12, meta=None, size=8, lower=8, upper=15, log=False), 'mlp_num_hidden': UniformIntegerHyperparameter(name='mlp_num_hidden', default_value=93, meta=None, size=115, lower=36, upper=150, log=False), 'mlp_init_std': UniformFloatHyperparameter(name='mlp_init_std', default_value=0.1409079900885, meta=None, size=inf, lower=0.088960498849, upper=0.192855481328, log=False), 'mlp_sparseness': 0.1449806273312999, 'mlp_input_sampling': 'uniform', 'mlp_output_noise': UniformFloatHyperparameter(name='mlp_output_noise', default_value=0.00085073221, meta=None, size=inf, lower=0.0003598301429, upper=0.0013416342771, log=False), 'mlp_noisy_targets': True, 'mlp_preactivation_noise_std': UniformFloatHyperparameter(name='mlp_preactivation_noise_std', default_value=0.0008449445554, meta=None, size=inf, lower=0.0003145707276, upper=0.0013753183831, log=False), 'input_warping_c1_std': 0.9759720822120248, 'input_warping_c0_std': 0.8002534583197192, 'num_hyperparameter_samples_per_batch': 16}}, 'num_features': 18, 'epoch_count': 0}\n",
      "Style definition of first 3 examples: None\n",
      "Initialized decoder for standard with (None, 1000)  and nout 1000\n",
      "Using a Transformer with 14.18 M parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(total_loss=inf, total_positional_losses=inf, model=TransformerModel(\n",
       "  (transformer_encoder): TransformerEncoderDiffInit(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): Normalize()\n",
       "    (1): VariableNumFeaturesEncoder(\n",
       "      (base_encoder): Linear(in_features=18, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (y_encoder): Linear(in_features=1, out_features=512, bias=True)\n",
       "  (pos_encoder): NoPositionalEncoding()\n",
       "  (decoder_dict): ModuleDict(\n",
       "    (standard): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): FullSupportBarDistribution()\n",
       "), data_loader=<pfns.priors.utils.get_batch_to_dataloader.<locals>.DL object at 0x11897ae90>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.train(**{**add_criterion(config_bnn,device='mps:0'), 'epochs': 1, 'steps_per_epoch': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b916abb-a847-4b2c-98be-a5d0e00922b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 384000 y evals to estimate 1000 buckets. Cut off the last 0 ys.\n",
      "Using cpu:0 device\n",
      "init dist\n",
      "Not using distributed\n",
      "DataLoader.__dict__ {'num_steps': 20, 'get_batch_kwargs': {'batch_size': 128, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x144acc720>, 'seq_len_maximum': 60, 'device': 'cpu:0', 'num_features': 18, 'hyperparameters': {'lengthscale_concentration': 1.2106559584074301, 'lengthscale_rate': 1.5212245992840594, 'outputscale_concentration': 0.8452312502679863, 'outputscale_rate': 0.3993553245745406, 'add_linear_kernel': False, 'power_normalization': False, 'hebo_warping': False, 'unused_feature_likelihood': 0.3, 'observation_noise': True}}, 'num_features': 18, 'epoch_count': 0}\n",
      "Style definition of first 3 examples: None\n",
      "Initialized decoder for standard with (None, 1000)  and nout 1000\n",
      "Using a Transformer with 26.79 M parameters\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 23.95s | mean loss  1.91 | pos losses   nan, 1.22, 2.25,  nan, 3.06, 1.53,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 1.44,  nan, 1.14, 1.85, 1.89, 1.97,  nan,  nan, 1.48, 1.84,  nan,  nan, 2.15,  nan,  nan, 2.23,  nan,  nan,  nan,  nan, 1.96,  nan,  nan,  nan,  nan, 1.75, 1.89,  nan,  nan, 2.95,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, lr 0.0 data time  0.02 step time  1.21 forward time  0.40 nan share  0.00 ignore share (for classification tasks) 0.0000\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingResult(total_loss=1.9092112183570862, total_positional_losses=[nan, 1.2173306941986084, 2.250448226928711, nan, 3.061293840408325, 1.5310368537902832, nan, nan, nan, nan, nan, nan, nan, nan, 1.4435828924179077, nan, 1.1444969177246094, 1.849971055984497, 1.8897478580474854, 1.9686771631240845, nan, nan, 1.4846199750900269, 1.8359873294830322, nan, nan, 2.154484272003174, nan, nan, 2.2339746952056885, nan, nan, nan, nan, 1.9593651294708252, nan, nan, nan, nan, 1.7461965084075928, 1.887776494026184, nan, nan, 2.949528217315674, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], model=TransformerModel(\n",
       "  (transformer_encoder): TransformerEncoderDiffInit(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): Normalize()\n",
       "    (1): VariableNumFeaturesEncoder(\n",
       "      (base_encoder): Linear(in_features=18, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (y_encoder): Linear(in_features=1, out_features=512, bias=True)\n",
       "  (pos_encoder): NoPositionalEncoding()\n",
       "  (decoder_dict): ModuleDict(\n",
       "    (standard): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): FullSupportBarDistribution()\n",
       "), data_loader=<pfns.priors.utils.get_batch_to_dataloader.<locals>.DL object at 0x1049c5010>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.train(**{**add_criterion(config_heboplus,device='mps:0'), 'epochs': 1, 'steps_per_epoch': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427f1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
